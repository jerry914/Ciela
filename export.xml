<?xml version="1.0" encoding="UTF-8"?>
<xml>
  <records>
    <record>
      <ref-type name="Journal Article"></ref-type>
      <contributors>
        <authors>
          <author>
            Lerchenfeldt, Sarah
          </author>
          <author>
            Taylor, Tracey AH
          </author>
        </authors>
        <secondary-authors>
        </secondary-authors>
      </contributors>
      <titles>
        <title>Best Practices in Peer Assessment: Training Tomorrow’s Physicians to Obtain and Provide Quality Feedback</title>
      </titles>
      <dates>
        <year>2020</year>
          <pub-dates>
            <date>8</date>
          </pub-dates>
      </dates>
      <volume>Volume 11</volume>
      <pages>571-578</pages>
      <electronic-resource-num>10.2147&#x2F;amep.s250761</electronic-resource-num>
      <abstract>Peer assessment, also known as peer feedback or peer evaluation, is a tool used in medical education for students to provide and receive constructive feedback. In undergraduate medical education, peer feedback is a method of assessment that is not used commonly; however, its use is on the rise. In this literature-based guide, we discuss the advantages of peer assessment, as well as tips for implementation (including training of students and faculty and assessment tools&#x2F;instruments) and strategies to overcome barriers to its use. Effective utilization of peer feedback can provide educators with an opportunity to evaluate attributes that are often difficult to assess, including professionalism, teamwork, work habits, and communication skills. Constructive feedback can raise learner awareness about performance and guide future decisions and action plans for improvement. Overall, when used appropriately, peer feedback can be a valuable and effective addition to the arsenal of assessments in medical education.</abstract>
      <publisher>Informa UK Limited</publisher>
      <periodical><full-title>Advances in Medical Education and Practice</full-title></periodical>
      <keywords>
      </keywords>
      <urls>
        <related-urls>
        </related-urls>
      </urls>
    </record>
    <record>
      <ref-type name="Journal Article"></ref-type>
      <contributors>
        <authors>
          <author>
            Guo, Xiuyan
          </author>
          <author>
            Liu, Jin
          </author>
          <author>
            Tang, Hengtao
          </author>
          <author>
            Gao, Ruiqin
          </author>
          <author>
            Chen, Qu
          </author>
          <author>
            Wolfer, Terry
          </author>
          <author>
            Haynes, Aisha
          </author>
        </authors>
        <secondary-authors>
        </secondary-authors>
      </contributors>
      <titles>
        <title>Development and validation of the peer assessment motivation scale (PAMS) in higher education</title>
      </titles>
      <dates>
        <year>2023</year>
      </dates>
      <volume>48</volume>
      <pages>1243-1257</pages>
      <issue>8</issue>
      <electronic-resource-num>10.1080&#x2F;03075079.2023.2191648</electronic-resource-num>
      <abstract>Measurement of students’ peer assessment motivation is critical to understand how they participate in such activities in higher education. The current study was conducted to develop and validate a brief scale that measures student peer assessment motivation in higher education using the Expectancy-Value Theory (EVT). Initial items were developed, revised, and administered to 369 students. Exploratory factor analyses suggested a three-factor model structure (ability belief, expectancy, and task value) aligning with EVT. Confirmatory factor analyses (n &#x3D; 399) supported a higher-order factor structure with the three first-order factors (i.e. ability belief, expectancy, and task value) with a decent model fit. The 20-items Peer Assessment Motivation Scale (PAMS) had decent internal reliability, test-retest reliability, convergent validity, and discriminant validity, suggesting that it is a high-quality measure. This scale is beneficial for instructors and researchers who are interested in investigating peer assessment motivation in higher education.</abstract>
      <publisher>Routledge</publisher>
      <periodical><full-title>Studies in Higher Education</full-title></periodical>
      <keywords>
        <keyword>Peer assessment motivation</keyword>
        <keyword>higher education</keyword>
        <keyword>instrument</keyword>
        <keyword>reliability</keyword>
        <keyword>validity</keyword>
      </keywords>
      <urls>
        <related-urls>
        </related-urls>
      </urls>
    </record>
    <record>
      <ref-type name="Journal Article"></ref-type>
      <contributors>
        <authors>
          <author>
            Sun, Dennis L.
          </author>
          <author>
            Harris, Naftali
          </author>
          <author>
            Walther, Guenther
          </author>
          <author>
            Baiocchi, Michael
          </author>
        </authors>
        <secondary-authors>
        </secondary-authors>
      </contributors>
      <titles>
        <title>Peer assessment enhances student learning: The results of a matched randomized crossover experiment in a college statistics class</title>
      </titles>
      <dates>
        <year>2015</year>
          <pub-dates>
            <date>12</date>
          </pub-dates>
      </dates>
      <volume>10</volume>
      <issue>12</issue>
      <accession-num>26683053</accession-num>
      <electronic-resource-num>10.1371&#x2F;journal.pone.0143177</electronic-resource-num>
      <abstract>Feedback has a powerful influence on learning, but it is also expensive to provide. In large classes it may even be impossible for instructors to provide individualized feedback. Peer assessment is one way to provide personalized feedback that scales to large classes. Besides these obvious logistical benefits, it has been conjectured that students also learn from the practice of peer assessment. However, this has never been conclusively demonstrated. Using an online educational platform that we developed, we conducted an in-class matched-set, randomized crossover experiment with high power to detect small effects. We establish that peer assessment causes a small but significant gain in student achievement. Our study also demonstrates the potential of web-based platforms to facilitate the design of high-quality experiments to identify small effects that were previously not detectable.</abstract>
      <publisher>Public Library of Science</publisher>
      <periodical><full-title>PLoS ONE</full-title></periodical>
      <keywords>
      </keywords>
      <urls>
        <related-urls>
        </related-urls>
      </urls>
    </record>
    <record>
      <ref-type name="Report"></ref-type>
      <contributors>
        <authors>
          <author>
            Faculty of Arts Arts Instructional Support &amp; Information Technology (Arts ISIT)
          </author>
        </authors>
        <secondary-authors>
        </secondary-authors>
      </contributors>
      <titles>
        <title>Ideas and Strategies for Peer Assessments</title>
      </titles>
      <keywords>
      </keywords>
      <urls>
        <related-urls>
          <url>https:&#x2F;&#x2F;isit.arts.ubc.ca&#x2F;ideas-and-strategies-for-peer-assessments&#x2F;</url>
        </related-urls>
      </urls>
    </record>
    <record>
      <ref-type name="Journal Article"></ref-type>
      <contributors>
        <authors>
          <author>
            Muramatsu, Kaito
          </author>
          <author>
            Oku, Takanori
          </author>
          <author>
            Furuya, Shinichi
          </author>
        </authors>
        <secondary-authors>
        </secondary-authors>
      </contributors>
      <titles>
        <title>The plyometric activity as a conditioning to enhance strength and precision of the finger movements in pianists</title>
      </titles>
      <dates>
        <year>2022</year>
          <pub-dates>
            <date>12</date>
          </pub-dates>
      </dates>
      <volume>12</volume>
      <issue>1</issue>
      <accession-num>36564388</accession-num>
      <electronic-resource-num>10.1038&#x2F;s41598-022-26025-0</electronic-resource-num>
      <abstract>Stability of timing and force production in repetitive movements characterizes skillful motor behaviors such as surgery and playing musical instruments. However, even trained individuals such as musicians undergo further extensive training for the improvement of these skills. Previous studies that investigated the lower extremity movements such as jumping and sprinting demonstrated enhancement of the maximum force and rate of force development immediately after the plyometric exercises. However, it remains unknown whether the plyometric exercises enhance the stability of timing and force production of the dexterous finger movements in trained individuals. Here we address this issue by examining the effects of plyometric exercise specialized for finger movements on piano performance. We compared the training-related changes in the piano-key motion and several physiological features of the finger muscles (e.g., electromyography, rate of force development, and muscle temperature) by well-trained pianists. The conditioning demonstrated a decrease of the variation in timing and velocity of successive keystrokes, along with a concomitant increase in the rate of force development of the four fingers, but not the thumb, although there was no change in the finger muscular activities through the activity. By contrast, such a conditioning effect was not evident following a conventional repetitive piano practice. In addition, a significant increase in the forearm muscle temperature was observed specifically through performing the plyometric exercise with the fingers, implying its association with improved performance. These results indicate effectiveness of the plyometric exercises for improvement of strength, precision, and physiological efficiency of the finger movements even in expert pianists, which implicates that ways of practicing play a key role in enhancing experts’ expertise.</abstract>
      <publisher>Nature Research</publisher>
      <periodical><full-title>Scientific Reports</full-title></periodical>
      <keywords>
      </keywords>
      <urls>
        <related-urls>
        </related-urls>
      </urls>
    </record>
    <record>
      <ref-type name="book_section"></ref-type>
      <contributors>
        <authors>
          <author>
            Panadero, Ernesto
          </author>
          <author>
            Jonsson, Anders
          </author>
          <author>
            Strijbos, Jan Willem
          </author>
        </authors>
        <secondary-authors>
        </secondary-authors>
      </contributors>
      <titles>
        <title>Scaffolding Self-Regulated Learning Through Self-Assessment and Peer Assessment: Guidelines for Classroom Implementation</title>
      </titles>
      <dates>
        <year>2016</year>
      </dates>
      <volume>4</volume>
      <pages>311-326</pages>
      <electronic-resource-num>10.1007&#x2F;978-3-319-39211-0_18</electronic-resource-num>
      <abstract>Although the focus on feedback and student involvement in Assessment for Learning (AfL) appears to align very well with theories of Self-Regulated Learning (SRL), and also seems to be the main reason for many researchers’ interest in formative assessment, the actual relationship between AfL and SRL is an issue of debate. In this chapter, we therefore explore the relationship between two AfL practices, namely, self-assessment and peer assessment, and SRL. These AfL practices emphasize student feedback and are both thought to increase student involvement in assessment. They also have evident connections to SRL models of self-regulation and co-regulation. Special attention is given to strategies for the implementation of peer and self-assessment in the classroom. In particular, guidelines are presented on teachers’ mediating and modeling role in peer and self-assessment, as well as on how to use formative assessment instruments, such as rubrics, scripts, and prompts, in order to promote student involvement in assessment.</abstract>
      <publisher>Springer Science and Business Media B.V.</publisher>
      <periodical><full-title>Enabling Power of Assessment</full-title></periodical>
      <keywords>
        <keyword>Forethought Phase</keyword>
        <keyword>Panadero</keyword>
        <keyword>Peer Assessment (PA)</keyword>
        <keyword>Provide Sufficient Time</keyword>
        <keyword>Rubra Group</keyword>
      </keywords>
      <urls>
        <related-urls>
        </related-urls>
      </urls>
    </record>
    <record>
      <ref-type name="Journal Article"></ref-type>
      <contributors>
        <authors>
          <author>
            Lim, Chee Leong
          </author>
          <author>
            Jalil, Habibah Ab
          </author>
          <author>
            Marof, Aini Marina
          </author>
          <author>
            Saad, Wan Zuhainis
          </author>
        </authors>
        <secondary-authors>
        </secondary-authors>
      </contributors>
      <titles>
        <title>Peer learning, self-regulated learning and academic achievement in blended learning courses: A structural equation modeling approach</title>
      </titles>
      <dates>
        <year>2020</year>
      </dates>
      <volume>15</volume>
      <pages>110-125</pages>
      <issue>3</issue>
      <electronic-resource-num>10.3991&#x2F;ijet.v15i03.12031</electronic-resource-num>
      <abstract>The ability for students to self-regulate their learning and to learn effectively with peers become two distinctive competencies in the era of the 4th Industrial Revolution. These competencies also affect academic achievement, an important variable used to measure attainment of learning outcomes. Therefore, this study was conducted to determine the influence of peer learning and selfregulated learning (SRL) strategies on students&#39; academic achievement. Of the 409 respondents, only 347 were valid for data analysis, forming a usable case of 84.84%. The instruments used was an online questionnaire, which was adapted from pre-existing reliable multi-item instruments. Structural Equation Model (SEM) analysis was used to examine the relationship between the constructs in the hypothesised model. Given that the structural model exhibited a good fit to the data (χ2&#x2F;df &#x3D; 1.697; CFI &#x3D; 0.916; IFI &#x3D; 0.917; TFI &#x3D; 0.912; and RMSEA &#x3D; 0.045), the results unveiled that students&#39; ability to learn with peers were found to have a positive and significant effect on academic achievement (β &#x3D; 0.478, C.R. &#x3D; 3.628, p &#x3D; 0.000), and significantly influenced students&#39; SRL strategies (β &#x3D; 0.793; C.R. &#x3D; 6.991; p &#x3D; 0.000). This study also discusses the practical implications to facilitate the development of students&#39; self-regulated learning (SRL) and peer learning competencies in blended learning courses.</abstract>
      <publisher>Kassel University Press GmbH</publisher>
      <periodical><full-title>International Journal of Emerging Technologies in Learning</full-title></periodical>
      <keywords>
        <keyword>Academic achievement</keyword>
        <keyword>Blended learning</keyword>
        <keyword>Peer learning</keyword>
        <keyword>Self-regulated learning (SRL)</keyword>
        <keyword>Structural equation modeling (SEM)</keyword>
      </keywords>
      <urls>
        <related-urls>
        </related-urls>
      </urls>
    </record>
    <record>
      <ref-type name="Journal Article"></ref-type>
      <contributors>
        <authors>
          <author>
            Double, Kit S.
          </author>
          <author>
            McGrane, Joshua A.
          </author>
          <author>
            Hopfenbeck, Therese N.
          </author>
        </authors>
        <secondary-authors>
        </secondary-authors>
      </contributors>
      <titles>
        <title>The Impact of Peer Assessment on Academic Performance: A Meta-analysis of Control Group Studies</title>
      </titles>
      <dates>
        <year>2020</year>
          <pub-dates>
            <date>6</date>
          </pub-dates>
      </dates>
      <volume>32</volume>
      <pages>481-509</pages>
      <issue>2</issue>
      <electronic-resource-num>10.1007&#x2F;s10648-019-09510-3</electronic-resource-num>
      <abstract>Peer assessment has been the subject of considerable research interest over the last three decades, with numerous educational researchers advocating for the integration of peer assessment into schools and instructional practice. Research synthesis in this area has, however, largely relied on narrative reviews to evaluate the efficacy of peer assessment. Here, we present a meta-analysis (54 studies, k &#x3D; 141) of experimental and quasi-experimental studies that evaluated the effect of peer assessment on academic performance in primary, secondary, or tertiary students across subjects and domains. An overall small to medium effect of peer assessment on academic performance was found (g &#x3D; 0.31, p &lt;.001). The results suggest that peer assessment improves academic performance compared with no assessment (g &#x3D; 0.31, p &#x3D;.004) and teacher assessment (g &#x3D; 0.28, p &#x3D;.007), but was not significantly different in its effect from self-assessment (g &#x3D; 0.23, p &#x3D;.209). Additionally, meta-regressions examined the moderating effects of several feedback and educational characteristics (e.g., online vs offline, frequency, education level). Results suggested that the effectiveness of peer assessment was remarkably robust across a wide range of contexts. These findings provide support for peer assessment as a formative practice and suggest several implications for the implementation of peer assessment into the classroom.</abstract>
      <publisher>Springer</publisher>
      <periodical><full-title>Educational Psychology Review</full-title></periodical>
      <keywords>
        <keyword>Effect size</keyword>
        <keyword>Experimental design</keyword>
        <keyword>Feedback</keyword>
        <keyword>Formative assessment</keyword>
        <keyword>Meta-analysis</keyword>
        <keyword>Peer assessment</keyword>
      </keywords>
      <urls>
        <related-urls>
        </related-urls>
      </urls>
    </record>
  </records>
</xml>